<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZZWR</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zanzong.github.io/"/>
  <updated>2019-08-28T07:16:46.888Z</updated>
  <id>http://zanzong.github.io/</id>
  
  <author>
    <name>zongzan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark History Server使用指南</title>
    <link href="http://zanzong.github.io/passages/c7d551e3.html"/>
    <id>http://zanzong.github.io/passages/c7d551e3.html</id>
    <published>2019-04-13T14:28:51.000Z</published>
    <updated>2019-08-28T07:16:46.888Z</updated>
    
    <content type="html"><![CDATA[<p>在使用Spark计算框架时，由于 Debug 、调优等的需要，常需要查看 Application 的运行时的详细信息。在 Spark 的 WebUI 上可以通过点击每个应用查看，而该应用执行完毕后，页面失效无法打开。Spark 提供了 <code>History Server</code> 来记录负载的详细运行时信息。</p><h1 id="启动HistoryServer"><a href="#启动HistoryServer" class="headerlink" title="启动HistoryServer"></a>启动HistoryServer</h1><ol><li><p>配置日志存放地址。 History Server将日志保存在 Spark 支持的文件系统，例如 HDFS。 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在HDFS创建用于存放日志的目录</span></span><br><span class="line">hdfs dfs -mkdir /sparklogs</span><br></pre></td></tr></table></figure></li><li><p>修改配置文件<br>配置文件 <code>spark-env.sh</code>，增加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置history server进程可使用的最大内存，默认1G</span></span><br><span class="line">SPARK_DAEMON_MEMORY=6g</span><br><span class="line"><span class="comment"># 设置logDirectory指向刚创建的目录</span></span><br><span class="line">SPARK_HISTORY_OPTS=<span class="string">"-Dspark.history.fs.logDirectory=hdfs://172.16.244.5:9000/sparklogs"</span></span><br></pre></td></tr></table></figure></li></ol><p>配置文件 <code>spark-defaults.conf</code>，增加<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置为true将使用spark.io.compression.codec配置的压缩方法进行压缩，默认为 lz4</span></span><br><span class="line">spark.eventLog.compress          <span class="literal">true</span></span><br><span class="line"><span class="comment"># 开启 eventLog</span></span><br><span class="line">spark.eventLog.enabled           <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir               hdfs://172.16.244.5:9000/sparklogs</span><br></pre></td></tr></table></figure></p><ol><li>启动 History Server<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-history-server.sh</span><br></pre></td></tr></table></figure></li></ol><p>启动后，可在Web页面<a href="http://master:18080（默认18080端口）看到如下内容" target="_blank" rel="noopener">http://master:18080（默认18080端口）看到如下内容</a><br><img src="/images/2019/historyserver1.jpg" alt></p><p>其余配置项可以参考<a href="https://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener">官方文档</a></p><h1 id="REST-API"><a href="#REST-API" class="headerlink" title="REST API"></a>REST API</h1><p>除了手工查看网页，在使用程序对Application自动监控时，也需要应用的详细执行信息。 History Server 提供使用REST API的方式访问网页, API 地址为 <code>http://&lt;server-url&gt;:18080/api/v1/applications/&lt;application&gt;</code>。 向该地址发送 HTTP 请求，Server 会返回 JSON 格式的信息。 该 API 包括详细的程序执行信息，支持包括applications、jobs、stages、executors等级别的信息查询，详见<a href="https://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener">官方文档</a>。<br>这里是使用 Python 查询某个application的简要信息的例子。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment"># 查询 app-20190828102157-0009 的提交和结束时间</span></span><br><span class="line">res = requests.get(<span class="string">"http://172.16.244.8:18080/api/v1/applications/app-20190828102157-0009"</span>)</span><br><span class="line">print(res.json())</span><br></pre></td></tr></table></figure></p><p>将输出以下内容<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"id"</span> : <span class="string">"app-20190828102157-0009"</span>,</span><br><span class="line">  <span class="attr">"name"</span> : <span class="string">"gbt-1566958883"</span>,</span><br><span class="line">  <span class="attr">"attempts"</span> : [ &#123;</span><br><span class="line">    <span class="attr">"startTime"</span> : <span class="string">"2019-08-28T02:21:56.000GMT"</span>,</span><br><span class="line">    <span class="attr">"endTime"</span> : <span class="string">"2019-08-28T02:27:24.061GMT"</span>,</span><br><span class="line">    <span class="attr">"lastUpdated"</span> : <span class="string">"2019-08-28T02:27:24.135GMT"</span>,</span><br><span class="line">    <span class="attr">"duration"</span> : <span class="number">328061</span>,</span><br><span class="line">    <span class="attr">"sparkUser"</span> : <span class="string">"ubuntu"</span>,</span><br><span class="line">    <span class="attr">"completed"</span> : <span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"endTimeEpoch"</span> : <span class="number">1566959244061</span>,</span><br><span class="line">    <span class="attr">"lastUpdatedEpoch"</span> : <span class="number">1566959244135</span>,</span><br><span class="line">    <span class="attr">"startTimeEpoch"</span> : <span class="number">1566958916000</span></span><br><span class="line">  &#125; ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在使用Spark计算框架时，由于 Debug 、调优等的需要，常需要查看 Application 的运行时的详细信息。在 Spark 的 WebUI 上可以通过点击每个应用查看，而该应用执行完毕后，页面失效无法打开。Spark 提供了 &lt;code&gt;History Serve
      
    
    </summary>
    
      <category term="大数据系统" scheme="http://zanzong.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Spark" scheme="http://zanzong.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>计算机系统常用性能指标</title>
    <link href="http://zanzong.github.io/passages/2785cf3a.html"/>
    <id>http://zanzong.github.io/passages/2785cf3a.html</id>
    <published>2019-03-24T11:21:24.000Z</published>
    <updated>2019-03-24T13:10:06.657Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Cycles-per-instruction（CPI）"><a href="#Cycles-per-instruction（CPI）" class="headerlink" title="Cycles per instruction（CPI）"></a>Cycles per instruction（CPI）</h3><p>表示每条计算机指令执行所需的时钟周期，简称指令的平均周期数，即<strong>一个指令被执行时所使用的时钟周期数量</strong>。对于一个程序，计算该程序执行完需要多少条机器指令（ins_num），并结合实际的硬件性能计算出需要多少个时钟周期来完成(cyc_num)<br>，可以得到该段程序在这个硬件上的CPI。对于不同的CPU，运行相同的负载得到的CPI是不同的，因此可以作为衡量不同CPU性能的一项指标。  <a id="more"></a><br>CPI = 执行程序所需要的时钟周期数 / 所执行的指令条数<br>实际上，大多数情况下一段程序会包含多种不同类型的指令，不同类型的指令运行需要不同的指令周期数，可使用如下公式计算<br>\begin{equation}\begin{split}<br>CPI = \frac{\sum_i (IC_i)(CC_i)}{IC}<br>\end{split}\end{equation}<br>其中${IC_i}$是指令类型为i的指令个数，${CC_i}$是完成指令类型i所需要的时钟周期（clock-cycles）个数。$IC=\sum_i{IC_i}$是所有类型指令个数的和。  </p><h3 id="Instructions-per-cycle（IPC）"><a href="#Instructions-per-cycle（IPC）" class="headerlink" title="Instructions per cycle（IPC）"></a>Instructions per cycle（IPC）</h3><p>即CPI的倒数</p><h3 id="Millions-of-instructions-per-second-（MIPS）"><a href="#Millions-of-instructions-per-second-（MIPS）" class="headerlink" title="Millions of instructions per second （MIPS）"></a>Millions of instructions per second （MIPS）</h3><p>Instructions per second（IPS），与CPI类似，也是一种衡量CPU性能的指标。它描述了指令的执行速率，规定了性能和执行时间成反比，即与IPC成正比，与CPI成反比。MIPS这种指标的缺点是没有考虑指令的能力，对于使用不同指令集的计算机，相同的程序产生的指令数是不同的，因此无法用MIPS来比较不同指令集的计算机，这个问题也出现在CPI和IPC以上两个指标上。<br>\begin{equation}\begin{split}<br>MIPS = \frac{指令数}{执行时间\times{10^6}} = \frac{时钟频率}{CPI\times{10^6}}<br>\end{split}\end{equation}  </p><p>引用用wiki上的一个例子：<br>一个400-MHz的处理器，运行一个基准测试负载，该负载所包含的指令信息以及指令周期数如下表所示  </p><div class="table-container"><table><thead><tr><th style="text-align:center">Instruction TYPE</th><th style="text-align:center">Instruction count</th><th style="text-align:center">Clock cycle count</th></tr></thead><tbody><tr><td style="text-align:center">Integer Arithmetic</td><td style="text-align:center">45000</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">Data transfer</td><td style="text-align:center">32000</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">Floating point</td><td style="text-align:center">15000</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">Control transfer</td><td style="text-align:center">8000</td><td style="text-align:center">2</td></tr></tbody></table></div><p>\begin{equation}\begin{split}<br>CPI = \frac{45000\times{1}+32000\times{2}+15000\times{2}+8000\times{2}}{100000} = 1.55<br>\end{split}\end{equation}  </p><p>400MHz = 400,000,000 Hz，假如用MIPS来衡量处理器性能，则有<br>\begin{equation}\begin{split}<br>MIPS = \frac{ClockFrequency}{CPI}\times\frac{1}{1 Million} = \frac{400,000,000}{1.55\times{1000000}} = 258 MIPS<br>\end{split}\end{equation}<br>执行所需的CPU时间为<br>\begin{equation}\begin{split}<br>Execution Time = CPI\times{Instruction count}\times{clocktime}\\<br>= \frac{CPI\times{instruction Count}}{ClockFrequency} = \frac{1.55\times{100000}}{400\times{1000000}} = \frac{1.55}{4000}\ <br>= 0.0003875 sec = 0.3785 ms<br>\end{split}\end{equation}  </p><h3 id="CACHE"><a href="#CACHE" class="headerlink" title="CACHE"></a>CACHE</h3><p>CPU要读取一个数据时，首先从Cache中查找，如果找到就立即读取并送给CPU处理；如果没有找到，就用相对慢的速度从内存中读取并送给CPU处理，同时把这个数据所在的数据块调入Cache中，可以使得以后对整块数据的读取都从Cache中进行，不必再调用内存。CPU在Cache中找到有用的数据被称为命中，当Cache中没有CPU所需的数据时（这时称为未命中），CPU才访问内存。为了保证CPU访问时有较高的命中率，Cache中的内容应该按一定的算法替换，其计数器清零过程可以把一些频繁调用后再不需要的数据淘汰出Cache，提高Cache的利用率。</p><p><a href="https://en.wikipedia.org/wiki/Cycles_per_instruction" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Cycles_per_instruction</a><br><a href="https://baike.baidu.com/item/CPU%E7%BC%93%E5%AD%98" target="_blank" rel="noopener">https://baike.baidu.com/item/CPU%E7%BC%93%E5%AD%98</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Cycles-per-instruction（CPI）&quot;&gt;&lt;a href=&quot;#Cycles-per-instruction（CPI）&quot; class=&quot;headerlink&quot; title=&quot;Cycles per instruction（CPI）&quot;&gt;&lt;/a&gt;Cycles per instruction（CPI）&lt;/h3&gt;&lt;p&gt;表示每条计算机指令执行所需的时钟周期，简称指令的平均周期数，即&lt;strong&gt;一个指令被执行时所使用的时钟周期数量&lt;/strong&gt;。对于一个程序，计算该程序执行完需要多少条机器指令（ins_num），并结合实际的硬件性能计算出需要多少个时钟周期来完成(cyc_num)&lt;br&gt;，可以得到该段程序在这个硬件上的CPI。对于不同的CPU，运行相同的负载得到的CPI是不同的，因此可以作为衡量不同CPU性能的一项指标。
    
    </summary>
    
      <category term="计算机系统" scheme="http://zanzong.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="性能指标" scheme="http://zanzong.github.io/tags/%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/"/>
    
  </entry>
  
  <entry>
    <title>Spark RDD 宽依赖窄依赖介绍</title>
    <link href="http://zanzong.github.io/passages/d3b4b981.html"/>
    <id>http://zanzong.github.io/passages/d3b4b981.html</id>
    <published>2019-03-24T11:20:29.000Z</published>
    <updated>2019-03-24T11:42:33.201Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文将介绍Spark RDD两种依赖关系——宽依赖、窄依赖，及其产生的相关特性</p></blockquote><hr><h2 id="Resilient-Distributed-Datasets"><a href="#Resilient-Distributed-Datasets" class="headerlink" title="Resilient Distributed Datasets"></a>Resilient Distributed Datasets</h2><p>RDD是一个容错、并行的数据结构，它可以直接将中间结果数据保存到内存中，通过控制数据分区优化存储，并提供丰富的数据操作接口<a href="http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf" target="_blank" rel="noopener">（nsdi2012）</a>。RDD的容错机制，使用一种粗粒度的transform来控制———RDD记录了如何产生结果数据的transform，而非实际的数据（即计算是lazy的），如果RDD的某个分区丢失，可以根据transform信息重新计算该partition来恢复丢失的数据。  <a id="more"></a>简言之，transformations使用lazy的操作定义了一个新的RDD,actions 触发计算执行并返回真正的结果数据。</p><h2 id="RDD-特性"><a href="#RDD-特性" class="headerlink" title="RDD 特性"></a>RDD 特性</h2><p>对于Apache Spark，RDD有如下的特性：</p><ul><li>RDD是只读对象</li><li>RDD可以通过加载本地文件系统或分布式文件系统中的数据获得（还可以通过数据+schema，通过SparkContext创建），或者通过其他RDD的transform变换得到。</li><li>数据的transformation操作是lazy的，Spark会根据transformation来构建DAG图，真正的计算执行由action来触发。</li><li>当内存不足时，RDD会溢出到磁盘上，并带来性能损失。</li><li>RDD具有容错机制，当一个RDD的部分partition丢失后，可以通过该RDD的<code>父RDD</code>重新计算该partition来恢复。</li><li>RDD适合用于实现批处理应用（Spark Streaming也是以micro batch的方式实现流计算模型）</li></ul><p>关于RDD的表达方式，作为一个抽象的数据对象，RDD使用以下5种信息来表达:  </p><ul><li>分区（partitions）：每个partition对应一个文件系统上的block，通过<code>partitions()</code>方法返回RDD的每个partition。  </li><li>依赖（dependencies）： 在父RDD上执行计算操作（function）来得到当前的RDD，通过<code>dependencies()</code>方法返回的依赖列表，获得该RDD的所有依赖。  </li><li>计算操作（function）：每个partition可以获得iterator对象，并实现作用于该迭代器的计算，完成特定的任务，如对分区中的每个元素map操作。  </li><li>分区方式（partition schema）：给出如何将数据划分到不同的partition的方法，Spark内置实现了两种partitioner——<code>HashPartitioner</code>和<code>RangePartitioner</code>。 </li><li>优先访问位置（data placement）：由于不同的partition可能存储在不同的机器，暴露出<code>preferredLocations(p)</code>方法来获得访问最快的数据存储节点列表，即partition p的优先访问位置。    </li></ul><h2 id="RDD依赖关系"><a href="#RDD依赖关系" class="headerlink" title="RDD依赖关系"></a>RDD依赖关系</h2><p>Spark将所有RDD构造成一个DAG图，RDD的容错机制依赖于lineage。如果一个RDD计算出错或丢失，那么可以从它的父RDD重新计算得到。所依赖的父RDD越少，这种recovery的代价就越小。根据数据是否可以以pipeline的方式在RDD之间转换，可以将依赖划分为宽依赖和窄依赖。<br><img src="/images/uploads/2018/08/rdd-deps-pic.png" alt></p><h4 id="窄依赖（narrow-dependencies）"><a href="#窄依赖（narrow-dependencies）" class="headerlink" title="窄依赖（narrow dependencies）"></a>窄依赖（narrow dependencies）</h4><p>父RDD的每个partition只能被一个子RDD所依赖，即父子partition的关系为<code>一对一</code>或<code>多对一</code>，例如map操作。<br>如上图左侧所示，常见的<code>map</code>、<code>filter</code>、<code>union</code>操作，不会影响RDD partition。对于输入已经进行协同划分(co-partitioned)的<code>join</code>操作，即数据已经事先按照key分组（也即是两个RDD有相同的Partitioner），也属于窄依赖。如图，进行协同划分后的两个父RDD，其子RDD的每个partiiton仅需依赖两个父RDD的partition。若子RDD<code>key1</code>所在分区计算失败，则仅仅需要重新计算该分区，拉取其依赖的两个partition。</p><blockquote><p>所谓协同划分，就是指定分区划分器以产生前后一致的分区安排。Pregel和HaLoop把这个作为系统内置的一部分；而Spark 默认提供两种划分器：HashPartitioner和RangePartitioner，允许程序通过partitionBy算子指定。Spark也允许用户自定义Partitioner来控制如何分区。</p></blockquote><h4 id="宽依赖（wide-dependencies）"><a href="#宽依赖（wide-dependencies）" class="headerlink" title="宽依赖（wide dependencies）"></a>宽依赖（wide dependencies）</h4><p>子RDD的每个partition都依赖于父RDD的所有partition，即存在<code>多对一</code>的关系，如groupByKey操作。<br>如上图右侧所示，在进行groupBy或无协同划分的join时，子RDD partition需要从父RDD的每个partition中拉取数据进行计算，即进行<code>shuffle</code>操作。分区对于shuffle操作很关键——如上图所示，同样对于join操作，进行协同划分后，两个父RDD之间、父RDD与子RDD之间能形成一致的分区，即保证了相同的key映射到同一分区，形成窄依赖；反之没有协同划分（上图右下），则形成宽依赖。</p><h4 id="Stage划分"><a href="#Stage划分" class="headerlink" title="Stage划分"></a>Stage划分</h4><p>在窄依赖的流程中，Spark可以以pipeline的方式计算各个RDD，即数据按照各部窄依赖操作向后流动，无需等每个节点计算完后再计算下一节点，大大提高计算性能。而一旦碰上宽依赖，该流水线则停止。例如执行groupByKey操作的RDD，需要每个分区都计算完成后，再进行shuffle操作，得到新的partition（上图右上）。<br>Spark将task划分为多个<code>stage</code>，而stage就是根据依赖关系来划分的。Spark DAGScheduler将回溯当前的RDD依赖图，每当遇到宽依赖，则将之前的task划分为一个stage，以此往复得到多个stage，每个stage中以pipeline的方式执行。因此每个stage的最后一个task往往是耗时的shuffle操作。</p><h4 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h4><p><em>分区丢失</em><br>基于lineage的实现非常有利于系统容错。假如发生节点宕机，丢失RDD A的partition 1，若是窄依赖，只需要求RDD A的父RDD存在，根据父RDD对应分区重算即可，跟其他节点没有依赖；而若是宽依赖，则需要所有父RDD的所有分区都存在，重新计算的代价较高。<br><em>设置检查点</em><br>如果RDD依赖的lineage较长，Spark提供API<code>rdd.checkpoint</code>使用checkpoint这个transform操作来做检查点。checkpoint操作会将该RDD数据写入到SparkContext中配置的CheckpointDir，持久化到磁盘（HDFS）。可以考虑在计算耗时的宽依赖设置检查点。  </p><h4 id="优化机制"><a href="#优化机制" class="headerlink" title="优化机制"></a>优化机制</h4><p><em>缓存</em><br>假如需要计算<code>rdd.count()</code>，若第一次执行，则Spark会将该rdd的所有依赖进行计算（注意RDD不是数据，只是记录着如何产生该数据），得到count的结果。但如果代码中再次调用<code>rdd.count()</code>呢？Spark会和上次一样重新查找rdd的依赖，计算出结果，造成了重复的计算。为了解决这种问题，Spark RDD暴露了两个接口<code>cache</code>和<code>persist</code>。其中，<code>persist</code>可以设置多种<a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence" target="_blank" rel="noopener">缓存级别</a>，而<code>cache</code>等同于<code>persist(MEMORY_ONLY)</code>。<br><em>广播变量</em><br>根据Spark官方文档，适合将“static look up tables”设置为广播变量。即在RDD计算过程中，可能需要一些静态的信息用于查询，如在map中访问一个人员信息的List，该List内容不可变。尽管在<code>map</code>中可以直接访问这些变量，但可能需要跨机器的访问这些数据，当数据量较大时，就产生了<code>shuffle</code>。因此，将一些不可变的数据，以broadcast的形式，分发到每个节点，使得查询过程避免了<code>shuffle</code>操作。</p><p>参考文献：<br>[1].<a href="http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf" target="_blank" rel="noopener">http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf</a><br>[2].<a href="https://blog.csdn.net/houmou/article/details/52531205" target="_blank" rel="noopener">https://blog.csdn.net/houmou/article/details/52531205</a><br>[3].<a href="http://www.beingsoftwareprofessional.com/2016/02/04/apache-spark-rdd/" target="_blank" rel="noopener">http://www.beingsoftwareprofessional.com/2016/02/04/apache-spark-rdd/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文将介绍Spark RDD两种依赖关系——宽依赖、窄依赖，及其产生的相关特性&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Resilient-Distributed-Datasets&quot;&gt;&lt;a href=&quot;#Resilient-Distributed-Datasets&quot; class=&quot;headerlink&quot; title=&quot;Resilient Distributed Datasets&quot;&gt;&lt;/a&gt;Resilient Distributed Datasets&lt;/h2&gt;&lt;p&gt;RDD是一个容错、并行的数据结构，它可以直接将中间结果数据保存到内存中，通过控制数据分区优化存储，并提供丰富的数据操作接口&lt;a href=&quot;http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;（nsdi2012）&lt;/a&gt;。RDD的容错机制，使用一种粗粒度的transform来控制———RDD记录了如何产生结果数据的transform，而非实际的数据（即计算是lazy的），如果RDD的某个分区丢失，可以根据transform信息重新计算该partition来恢复丢失的数据。
    
    </summary>
    
      <category term="大数据系统" scheme="http://zanzong.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Spark" scheme="http://zanzong.github.io/tags/Spark/"/>
    
      <category term="大数据" scheme="http://zanzong.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>我陪着你呀</title>
    <link href="http://zanzong.github.io/passages/29bba495.html"/>
    <id>http://zanzong.github.io/passages/29bba495.html</id>
    <published>2019-03-24T11:17:16.000Z</published>
    <updated>2019-08-28T02:12:31.734Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Enter password to read." />    <label for="pass">Enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Enter password to read.
    
    </summary>
    
      <category term="生活" scheme="http://zanzong.github.io/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="日常" scheme="http://zanzong.github.io/tags/%E6%97%A5%E5%B8%B8/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hadoop YARN 产生背景和需求</title>
    <link href="http://zanzong.github.io/passages/6c658b9d.html"/>
    <id>http://zanzong.github.io/passages/6c658b9d.html</id>
    <published>2019-03-24T10:25:44.000Z</published>
    <updated>2019-03-24T12:41:16.893Z</updated>
    
    <content type="html"><![CDATA[<hr><p>本文主要翻译自论文：<br><a href="https://www.sics.se/~amir/files/download/dic/2013%20-%20Apache%20Hadoop%20YARN:%20Yet%20Another%20Resource%20Negotiator%20%28SoCC%29.pdf" target="_blank" rel="noopener">Apache Hadoop YARN: Yet Another Resource Negotiator</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Hadoop设计之初，目的在于使用MapReduce作业，处理大量的网络数据。得益于快速增长的各类科技公司,Hadoop成为了数据和计算资源的“市场”——数据和计算资源共享。广泛而普遍的使用，超出了它最初的设计目标，让它暴露出两个关键的缺点：</p><blockquote><ol><li>编程模型（MapReduce）和资源管理框架之间紧密的联系，迫使开发者滥用MapReduce模型。</li><li>集中式的作业流程控制（JobTrack和TaskTrack），使得调度的面临扩展问题（集群job数量越大越明显）</li></ol></blockquote><a id="more"></a><p>在这篇文章中，总结了关于下一代Hadoop资源管理框架YARN的设计、开发以及部署情况。新的架构，从资源管理的功能中解耦了编程模型（mapreduce），并且增加了许多针对application的调度功能。并由相关的实验数据，证明了该架构确实行之有效，实验数据来源于Yahoo的Hadoop集群。并通过运行在YARN上的计算框架，例如Dryad、Giraph、Hoya、Hadoop MapReduce、REEF、Spark、Storm、Tez等，证明YARN具有良好的扩展性。</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>Apache Hadoop是一个开源的实现了MapReduce模型的计算框架之一，最初主要解决<strong>搜索引擎面临的海量的数据扩展性差</strong>的问题，并着重于数据密集型计算的<strong>容错性</strong>的问题，被很多大型网络公司和创业公司所使用。更重要的是，它成为了一个工程是和研究人员获得计算资源和存储资源的平台。这导致了Hadoop的成功，也带来了很多骂声，因为很多开发者滥用MapReduce导致了程序过多的使用了集群资源。举个栗子，一个常见的场景就是提交“map-only”的作业时，产生大量map task个而不考虑集群资源，还有就是web server这种长的负载以及成群的需要迭代计算的负载。而开发者为了自己获得足够的物理资源，常常通过一些方法来回避MapReduce API的限制，去占用了过多的资源。</p><p>正是这些局限性和资源的滥用，产生了以Hadoop为基础的环境无关方面的论文，很多论文暴露了Hadoop架构和实现方面的问题，以及资源滥用来带的负面影响。并且这些Hadoop架构中的局限性已经被学术界和开源社区所认识。</p><p>本文中，展示了一些由社区主导的来推动Hadoop发展的成果，以及下一代Hadoop计算平台组件YARN。它是从之前整体的架构中，将之前对于每个作业调度的资源管理功能从编程模型中分离出来。在这种新的架构中，MapReduce运行在YARN上的众多应用中的一种，使得在选择计算框架时有很大的灵活性而不仅限于Hadoop MapReduce，并且可以动态的调整以获得性能（可能是资源利用率）的提高。</p><h2 id="2-历史"><a href="#2-历史" class="headerlink" title="2 历史"></a>2 历史</h2><p>YARN的需求是在Hadoop MapReduce发展过程中产生的。随着集群规模的扩大和用户的增多，逐渐暴露出来的弊端促进了Hadoop MapReduce的发展，其中的经验和问题催生了YARN，列举如下：</p><blockquote><ol><li>扩展性</li><li>多租户</li><li>可维护性</li><li>存储位置敏感</li><li>高集群利用率</li><li>可用性</li><li>安全和审计操作的功能</li><li>多样化的编程模型</li><li>灵活的资源模型</li><li>向后兼容性</li></ol></blockquote><p>在实践中YARN的需求时怎样产生的呢？从2006年起，Yahoo！开始采用Apache Hadoop作为基础设施来运行其“WebMap”应用，它构建了一个web网站之间的<code>图（graph）</code>结构，该应用为其搜索引擎提供服务。同时，该网站图（web graph）包含了超过一千亿个节点和一万亿的边。之前名为<code>Dreadnaught</code>的底层基础设施在800台机器的集群中已经遇到了扩展的瓶颈，因此需要一个代替它的框架。Dreadnaught是不能满足类似于MapReduce的分布式作业的，因此需要采用扩展性较好的MapReduce框架，来实现搜索业务的顺利迁移。因此<strong>扩展性</strong>成为了早期Hadoop版本以至于后来的YARN着重考虑的因素。</p><p>除了雅虎搜索引擎大规模的流水作业，在广告分析优化、垃圾邮件过滤、内容优化等方面的需求也驱动了Hadoop发展。同时，Hadoop社区为了扩展Hadoop平台以适应更大规模的MapReduce作业，<strong>多租户</strong>的目标开始成型。在这个过程中，大家也更好的理解了工程开发的优先级和一些中间的阶段。</p><h3 id="2-1-专用集群的时代"><a href="#2-1-专用集群的时代" class="headerlink" title="2.1 专用集群的时代"></a>2.1 专用集群的时代</h3><p>一些早期的Hadoop用户将集群部署在少数节点上，从HDFS加载数据，通过MapReduce计算数据并拉取结果。随着Hadoop的容错机制的完善，将数据持久化到HDFS成为了一个标准。而在雅虎，工程师将加载数据集到一个共享的集群中，这吸引了研究人员兴趣。尽管大规模的计算仍然是开发Hadoop的主要动力，然而HDFS也需要权限模型，分配机制等特性来提高多租户使用的能力。</p><p>为了定位一些多租户使用的问题，雅虎开发并部署了Hadoop on Demand（<code>HOD</code>），它使用<a href="http://www.adaptivecomputing.com/products/open-source/torque/" target="_blank" rel="noopener"><code>Torque</code></a>和<a href="http://www.adaptivecomputing.com/products/open-source/maui/" target="_blank" rel="noopener"><code>Maui</code></a>共享集群物理资源并分配给Hadoop。用户可以向Torque在共享资源池中为作业申请节点，作业会被提交到队列中等待，直到有足够节点可用时才开始运行。当作业运行时，HoD会在主节点启动一个leader进程，它会和Torque，Maui通信并启动slave，随后启动<code>JobTracker</code>和<code>TaskTracker</code>。当用户使用完资源后，节点会被释放，重新回到资源池中。因为HoD为每个作业都启动一个新的集群，所以开发者在测试新版本Hadoop的同时，旧版本Hadoop的运行不受影响。然而Hadoop每三个月就会更新一次主版本，HoD的灵活性难以跟得上这个节奏，于是需要对HoD进行解耦升级工作——增加<strong>可维护性</strong>。</p><p>由于HoD中也可以部署HDFS集群，大多数用户部署计算节点时使用了共享的HDFS实例。随着HDFS集群规模扩展，越来越多的计算节点运行在HDFS的服务之上，提高了数据的用户密度，形成了一个良性循环，打开了一块新的天地。</p><p>实践证明HoD是一个通用框架，它拥有一些<code>Mesos</code>的特性，扩展了framework-master模型，来支持多样的并发编程模型的动态资源分配。它也可以被认为是私有云先驱者，例如AWS的EC2 Elastic MapReduce，微软的Azure HDInsight</p><h3 id="2-2-Hadoop不能满足需求"><a href="#2-2-Hadoop不能满足需求" class="headerlink" title="2.2 Hadoop不能满足需求"></a>2.2 Hadoop不能满足需求</h3><p>雅虎最后由于平庸的集群资源利用率而淘汰了HoD。在<code>map</code>阶段，JobTracker力求task放在离输入数据最近的节点上，最近的当然就是存储该数据的HDFS块所在的节点上。但是由于Torque在分配节点时没有考虑数据位置，分配给用户JobTracker的节点中，可能只包含了部分的数据副本，因此很可能这台机器上并没有task所需的输入数据，于是就需要从其他节点上拉取数据。当作业由大量的task都需要这样的时候，资源争抢会异常激烈。当TaskTracker分布的各个机架时，更增加了跨机架读取数据的可能性，并且在<code>shuffle</code>的时候必然要跨机架的拉取数据，使得按DAG执行的后面的作业也如此。因此关于<strong>存储的位置敏感</strong>（Locality awareness），也是YARN的关键需求之一。</p><p>高层的框架，例如Pig、Hive常常构造MapReduce的工作流成为一个有向无环图（<code>DAG</code>），在计算的每个阶段过滤、聚和、转换数据。因为在使用HoD时，集群大小在创建后是不能调整的，大量的集群资源在等待其他作业执行时是空闲的。一个极端但是却常见的情况就是，只在一个节点上运行的<code>reduce task</code>可能会因为运行时间过长导致该节点资源一直无法被收回，而其他节点此时是空闲的。</p><p>最后，作业响应时间由集群分配时间主导的，而用户很难判断作业到底需要多少节点去执行，常常凭直觉去申请数以十倍的资源。而集群分配时间又很长，以至于用户分配到资源后常常和同事一起使用，这使得这些资源一直被占着无法释放，从而形成让资源分配时间更长的恶性循环。尽管如此，用户也深爱着HoD的一些特性，从经济的角度考量，使得雅虎必须让员工使用共享的集群资源。因此，<strong>高集群利用率</strong>成了YARN的一个高优先级需求。</p><h3 id="2-3-共享集群"><a href="#2-3-共享集群" class="headerlink" title="2.3 共享集群"></a>2.3 共享集群</h3><p>最后，HoD已经无法合理的在资源分配时做出选择，资源分配的粒度太粗，并且它的API迫使用户写一些令人误解的对申请资源的约束条件。然而，向共享集群前进的意义是巨大的。尽管HDFS的规模在过去几年中逐渐扩大，上层的JobTracker却已经被<code>HoD</code>隔离开了。当去掉<code>HoD</code>之后，MapReduce集群规模突然增大，作业吞吐量剧烈增加，一些特性被平白无故的加入到JobTracker中，为一些大bug的埋下了隐患。更糟糕的，一个JobTracker的失败可导致运行中断，并且该集群中所有的作业需要用户手动重新按工作流执行，而之前只是会丢掉当前这一个工作流程的结果。</p><p>集群停掉之后，作业流水线上积累了很多待执行的作业，当集群重启后，JobTracker的压力猛增。以至于重启后常常需要手动杀死一些用户的作业，直到集群被正常的启动。由于每个作业存储的复杂性，在集群重启时保护已提交作业的实现方案一直存在着BUG。</p><p>维护一个多租户的的Hadoop集群是很困难的。当很多接口暴露给用户时，<code>容错能力</code>成了核心的设计原则。针对单点失败，暴露出各种关于可用性的问题，并指出持续的在集群中监控作业是有争议的。具体来说，因为JobTracker需要为每个作业分配监进程的资源，它的访问控制逻辑包含了安全机制来确保自身的可用性；它可能延迟分配出可用资源，因为JobTracker为了监控作业使用了过多的资源。这些都可归结到<strong>可用性</strong>方面上。</p><p>当Hadoop集群的租户增多，并且提交作业类型以及数据来源多种多样，于是资源隔离的问题凸显出来。但是授权模型不够强大，并且没有良好的扩展性，这是多租户集群的一个关键问题。<strong>安全和审计操作</strong>的功能必须在YARN中保留。开发者逐渐增强了这个系统，使其不同于以往基于<code>槽</code>（slot）的资源管理，来适应多样化的资源需求。</p><p>虽然MapReduce支持广泛的使用场景，但并不是一个对所有大规模计算都理想的模型。例如，很多机器学习算法的需要多次的迭代才能得出结果。如果一系列的Mapreduce作业包含这种计算，无疑调度的开销会延迟结果的产生。类似的，<a href="https://en.wikipedia.org/wiki/Bulk_synchronous_parallel" target="_blank" rel="noopener"><code>BSP</code></a>（bulk-synchronous parallel model）模型可以更好的表示许多图算法。 相比于大规模的MapReduce作业的容错机制中的all-to-all的交互障碍，图的节点之间的交互是更合适的。（<em>all-to-all:from a Map stage to the next Reduce stage, one-to-one:from a Reduce stage to the next Map stage.</em><a href="http://dprg.cs.uiuc.edu/docs/iss-socc/socc077-ko.pdf" target="_blank" rel="noopener">另一篇论文</a>）。这种不协调阻碍了用户的生产力，然而MapReduce的中心资源模型是<code>无资源竞争模型</code>。Hadoop在Yahoo的广泛部署以及其数据流水线的重要性加剧了这种矛盾，更糟糕的是，用户通常使用多种框架编写MapReduce程序。对于调度器而言，这种作业被视为<code>map-only</code>的作业，它们常常有完全不同的资源使用曲线，使平台的对作业的预估不准确，拉低了资源利用率，容易产生死锁并增大了不稳定性。因此，YARN必须支持<strong>多样化的编程模型</strong>。</p><p>除了与新型框架需求的不匹配之外，<code>槽</code>的类型固定也会损害利用率。尽管将槽划分为map槽和reduce槽可以防止槽死锁，但这也成为了资源使用的瓶颈。在Hadoop中，由用户为每个作业设置两个阶段的重叠部分，晚启动reduce作业可以增加集群的吞吐量，而早启动reduce作业可以减少执行的时延。map槽和reduce槽的数量是由集群管理员设定的，map槽不能执行reduce task，反之，reduce槽也不能执行map task。因为map task和reduce task的比例通常是不同的，所以没有一种配置能够完美的平衡。任意一种槽出现饱和，<code>JobTracker</code>就很难再初始化新的作业并顺利执行。可实现复杂的调度但又不浪费集群的资源，这突出了对<strong>灵活的资源模型</strong>的需求。</p><p>尽管相比于<code>HoD</code>，共享集群提高了资源利用率，它也引起了对可维护性和可用性的担忧。在集群中部署新版本Hadoop是一件需要很谨慎也很平常的事儿，当修复了MapReduce实现中的bug，需要重启集群再提交作业。通过将资源管理平台和编程框架合并，使它们都得到了发展；当用户提升了平台分配资源的效率，与平台整合的框架也要相应的改变。尽管升级通常仅仅只需要重新编译，但用户的对框架内部细节的一些假设（或者开发者对用户程序的一些假设）也可能会产生一些不兼容。</p><p><center>![](/images/uploads/2017/04/20170409110122.jpg)图一 YARN架构（蓝色的是YARN的系统组件，黄色和粉色是两个运行在YARN上的应用）</center><br>建立在Apache Hadoop MapReduce发展的基础上，YARN就是为了解决这些需求（R1-R9）。然而有大量的已经部署的MapReduce应用，以及Hadoop生态系统中相关的项目，使得YARN在设计需要保持兼容性，新架构需要尽量重用之前框架的代码。这也是YARN的最后一个需求：<strong>向后兼容性</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;本文主要翻译自论文：&lt;br&gt;&lt;a href=&quot;https://www.sics.se/~amir/files/download/dic/2013%20-%20Apache%20Hadoop%20YARN:%20Yet%20Another%20Resource%20Negotiator%20%28SoCC%29.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Apache Hadoop YARN: Yet Another Resource Negotiator&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;Hadoop设计之初，目的在于使用MapReduce作业，处理大量的网络数据。得益于快速增长的各类科技公司,Hadoop成为了数据和计算资源的“市场”——数据和计算资源共享。广泛而普遍的使用，超出了它最初的设计目标，让它暴露出两个关键的缺点：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;编程模型（MapReduce）和资源管理框架之间紧密的联系，迫使开发者滥用MapReduce模型。&lt;/li&gt;
&lt;li&gt;集中式的作业流程控制（JobTrack和TaskTrack），使得调度的面临扩展问题（集群job数量越大越明显）&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://zanzong.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://zanzong.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="资源调度" scheme="http://zanzong.github.io/tags/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/"/>
    
  </entry>
  
</feed>
